{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from india_calendar.ipynb\n",
      "importing Jupyter notebook from backtest.ipynb\n",
      "importing Jupyter notebook from feeds.ipynb\n",
      "importing Jupyter notebook from synfeed.ipynb\n",
      "importing Jupyter notebook from featfuncs.ipynb\n",
      "importing Jupyter notebook from feed_env.ipynb\n",
      "importing Jupyter notebook from rlagents.ipynb\n",
      "importing Jupyter notebook from aiagentbase.ipynb\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO,A2C,DQN\n",
    "from stable_baselines3.common.vec_env import StackedObservations\n",
    "from stable_baselines3.common.monitor import Monitor as Mon\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import import_ipynb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datetime import datetime\n",
    "from datetime import datetime as dt\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from india_calendar import IBDay\n",
    "from threading import Thread\n",
    "import threading\n",
    "from IPython import display\n",
    "from backtest import Backtest\n",
    "from rlagents import RLStratAgentDyn, COLS, RLStratAgentDynFeatures\n",
    "import time,getopt,sys,os\n",
    "\n",
    "from feeds import BackFeed,DataFeed\n",
    "from featfuncs import feat_aug,add_addl_features_feed,add_ta_features_feed,add_sym_feature_feed\n",
    "from featfuncs import add_global_indices_feed\n",
    "\n",
    "from feed_env import Episode\n",
    "import aspectlib\n",
    "import yaml\n",
    "import pickle\n",
    "import plotly.express as px\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def check_numeric(df, col):\n",
    "    return df[col].dtype in ['float64', 'int64']\n",
    "\n",
    "def difference_cols(df, a, b):\n",
    "    df[f'{a}-{b}'] = df[a] - df[b]\n",
    "    return df, f'{a}-{b}'\n",
    "\n",
    "def get_ma_base_string(s):\n",
    "    idx = s.find('_ma_')\n",
    "    if idx == -1:\n",
    "        return None\n",
    "    return s[:idx]\n",
    "\n",
    "def moving_avg(df, col, window_size=3, center=False):\n",
    "    col_name = f'{col}_ma_{window_size}'\n",
    "    df[col_name] = df[col].rolling(window_size, min_periods=1, center=center).mean()\n",
    "    return df, col_name\n",
    "\n",
    "def slope(df, col, window):\n",
    "    col_name = f'{col}_slope_{window}'\n",
    "    df[col_name] = df[col].diff(periods=window).fillna(df[col])/window\n",
    "    return df, col_name\n",
    "\n",
    "def max_change_helper(seq):\n",
    "    ans = []\n",
    "    tracker = {i:0 for i in range(seq[-1]+1)}\n",
    "    for i in seq:\n",
    "        tracker[i] += 1\n",
    "        ans.append(tracker[i])\n",
    "    return ans\n",
    "\n",
    "def max_change(df, col):\n",
    "    inc_tracker = df[col].diff().lt(0).cumsum().values\n",
    "    dec_tracker = df[col].diff().gt(0).cumsum().values\n",
    "    \n",
    "    inc_values = max_change_helper(inc_tracker)\n",
    "    dec_values = max_change_helper(dec_tracker)\n",
    "    \n",
    "    combined = [inc_values[i]-1 if inc_values[i] >= dec_values[i] \\\n",
    "                else -dec_values[i]+1 for i in range(len(inc_values))]\n",
    "    \n",
    "    col_name = f'{col}_changelen'\n",
    "    df[col_name] = combined\n",
    "    return df, col_name\n",
    "\n",
    "def discretize(df, col, thresholds):\n",
    "    stats = df[col].describe()\n",
    "    low_thresh, high_thresh = stats['25%'], stats['75%']\n",
    "    thresholds[col] = (low_thresh, high_thresh)\n",
    "    df[f'{col}_val'] = df[col].apply(lambda x: 0 if x<=low_thresh else 2 if x>=high_thresh else 1)\n",
    "    df[f'{col}_polarity'] = df[col].apply(lambda x: 1 if x>0 else -1)\n",
    "    # df[f'{col}_discrete'] = df[f'{col}_val'] + df[f'{col}_polarity']\n",
    "    return df, [f'{col}_val', f'{col}_polarity'] #, f'{col}_discrete'\n",
    "\n",
    "def add_features(feed, columns_to_use=None, do_discretization=True):\n",
    "    thresholds = {}\n",
    "    if columns_to_use is None:\n",
    "        columns_to_use = ['Open', 'High', 'Low', 'Close', 'Volume', 'row_num', 'Open_n', \n",
    "                    'High_n', 'Low_n', 'Close_n', 'Volume_n', 'SMA_10',\n",
    "        'SMA_20', 'VOL_SMA_20', 'RSI_14', 'BBL_5_2.0', 'BBM_5_2.0', 'BBU_5_2.0',\n",
    "        'BBB_5_2.0', 'BBP_5_2.0', 'MACD_12_26_9', 'MACDh_12_26_9',\n",
    "        'MACDs_12_26_9', 'VWAP_D', 'MOM_30', 'CMO_14']\n",
    "        \n",
    "    subtract_col_names = [('High', 'Low'), ('Open', 'Close'), ('SMA_20', 'SMA_10'), ('Open_n', 'Close_n'), ('High_n', 'Low_n'), ('Open', 'High')]\n",
    "    subtract_cols = []\n",
    "\n",
    "    for cols in subtract_col_names:\n",
    "        if cols[0] not in columns_to_use or cols[1] not in columns_to_use:\n",
    "            continue\n",
    "        feed, added_col = difference_cols(feed, cols[0], cols[1])\n",
    "        subtract_cols.append(added_col)\n",
    "        \n",
    "    window_sizes = [1,5,10,20,50]\n",
    "    pre_avg_cols = columns_to_use + subtract_cols\n",
    "    avg_cols = []\n",
    "\n",
    "    for window in window_sizes:\n",
    "        for col in pre_avg_cols:\n",
    "            feed, added_col = moving_avg(feed, col, window_size=window)\n",
    "            avg_cols.append(added_col)\n",
    "                \n",
    "    pre_slope_cols = columns_to_use + subtract_cols + avg_cols\n",
    "    window_sizes = [1,3,5,10,15]\n",
    "    slope_cols = []\n",
    "\n",
    "    for window in window_sizes:\n",
    "        for col in pre_slope_cols:\n",
    "            feed, added_col = slope(feed, col, window=window)\n",
    "            slope_cols.append(added_col)\n",
    "            \n",
    "    intra_ma_diff_cols = []\n",
    "\n",
    "    for i in range(len(avg_cols)-1):\n",
    "        for j in range(i+1, len(avg_cols)):\n",
    "            colA, colB = avg_cols[i], avg_cols[j]\n",
    "            baseA, baseB = get_ma_base_string(colA), get_ma_base_string(colB)\n",
    "            if baseA != baseB: continue\n",
    "            \n",
    "            feed, added_col = difference_cols(feed, colA, colB)\n",
    "            intra_ma_diff_cols.append(added_col)\n",
    "            \n",
    "    pre_change_cols = columns_to_use + subtract_cols + avg_cols + slope_cols + intra_ma_diff_cols\n",
    "    change_cols = []\n",
    "\n",
    "    for col in pre_change_cols:\n",
    "        feed, added_col = max_change(feed, col)\n",
    "        change_cols.append(added_col)\n",
    "        \n",
    "    pre_discrete_cols = pre_change_cols + change_cols\n",
    "    discrete_cols = []\n",
    "\n",
    "    if do_discretization:\n",
    "        for col in pre_discrete_cols:\n",
    "            feed, added_cols = discretize(feed, col, thresholds)\n",
    "            for added_col in added_cols: \n",
    "                discrete_cols.append(added_col)\n",
    "    else:\n",
    "        pass\n",
    "        # Use the stored thresholds\n",
    "        \n",
    "    return feed, pre_discrete_cols, discrete_cols, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringify(x):\n",
    "    return pd.to_datetime(x['Datetime']).strftime('%d-%b-%Y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_thresh = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('additional_utils/cols.pkl', 'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "imp_cols = d['imp_cols']\n",
    "cols_to_use = d['cols_to_use']\n",
    "prediscrete_imp_cols = d['prediscrete_imp_cols']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_cols_n = list(set(['Open_n-Close_n_ma_1-Open_n-Close_n_ma_10_val',\n",
    " 'Close_n_slope_1_polarity',\n",
    " 'Volume_n_ma_10_val',\n",
    " 'Open_n-Close_n_slope_1_val',\n",
    " 'MACDs_12_26_9_ma_20_changelen_val',\n",
    " 'Open_n-Close_n_ma_1-Open_n-Close_n_ma_50_val',\n",
    " 'BBP_5_2.0_slope_1_changelen_val',\n",
    " 'BBM_5_2.0_ma_1-BBM_5_2.0_ma_20_changelen_val',\n",
    " 'Open_n-High_n_ma_5-Open_n-High_n_ma_20_changelen_val',\n",
    " 'BBL_5_2.0_ma_5_slope_1_changelen_polarity',\n",
    " 'High_n-Low_n_ma_50_changelen_val',\n",
    " 'High_n-Low_n_ma_20_slope_15_changelen_val',\n",
    " 'BBL_5_2.0_ma_10_slope_15_val',\n",
    " 'Open_n-Close_n_ma_10_val',\n",
    " 'Volume_n_ma_5_slope_5_val',\n",
    " 'Close_n_slope_1_polarity',\n",
    " 'Low_n_ma_5_slope_3_polarity',\n",
    " 'Open_n_ma_10_slope_5_changelen_polarity',\n",
    " 'RSI_14_ma_1_val',\n",
    " 'Open_n-Close_n_polarity',\n",
    " 'Open_n-High_n_ma_1-Open_n-High_n_ma_50_val',\n",
    " 'Open_n-Close_n_ma_1-Open_n-Close_n_ma_20_val',\n",
    " 'Close_n_ma_1-Close_n_ma_50_changelen_polarity',\n",
    " 'High_n_ma_1-High_n_ma_20_changelen_polarity',\n",
    " 'Open_n-High_n_ma_1-Open_n-High_n_ma_50_polarity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "d['imp_cols_n'] = imp_cols_n\n",
    "\n",
    "with open('additional_utils/cols.pkl', 'wb') as f:\n",
    "        pickle.dump(d, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading datafile: ../algodata/realdata/alldata.csv\n",
      "['NELCAST.NS', 'AARTIDRUGS.NS', 'CYIENT.NS', 'HINDZINC.NS', 'TRENT.NS', 'TATAELXSI.NS', 'MINDTREE.NS', 'TI.NS', 'OBEROIRLTY.NS', 'SOMANYCERA.NS']\n",
      "Creating feed\n",
      "['NTPC.NS', 'DCM.NS', 'ACC.NS', 'TECHM.NS', 'MRF.NS', 'SUNPHARMA.NS', 'BPCL.NS', 'RELIANCE.NS', 'MUTHOOTFIN.NS', 'HINDUNILVR.NS', 'HDFC.NS', 'POWERGRID.NS', 'WIPRO.NS', 'PFC.NS', 'PETRONET.NS', 'DRREDDY.NS', 'ABBOTINDIA.NS', 'TCS.NS', 'BOSCHLTD.NS', 'KOTAKBANK.NS', 'SBIN.NS', 'MARUTI.NS']\n",
      "Processing feed\n",
      "Adding features to feed!\n",
      "[INFO] ON ticker=NTPC.NS\n",
      "[INFO] ON ticker=DCM.NS\n",
      "[INFO] ON ticker=ACC.NS\n",
      "[INFO] ON ticker=TECHM.NS\n",
      "[INFO] ON ticker=MRF.NS\n",
      "[INFO] ON ticker=SUNPHARMA.NS\n",
      "[INFO] ON ticker=BPCL.NS\n",
      "[INFO] ON ticker=RELIANCE.NS\n",
      "[INFO] ON ticker=MUTHOOTFIN.NS\n",
      "[INFO] ON ticker=HINDUNILVR.NS\n",
      "[INFO] ON ticker=HDFC.NS\n",
      "[INFO] ON ticker=POWERGRID.NS\n",
      "[INFO] ON ticker=WIPRO.NS\n",
      "[INFO] ON ticker=PFC.NS\n",
      "[INFO] ON ticker=PETRONET.NS\n",
      "[INFO] ON ticker=DRREDDY.NS\n",
      "[INFO] ON ticker=ABBOTINDIA.NS\n",
      "[INFO] ON ticker=TCS.NS\n",
      "[INFO] ON ticker=BOSCHLTD.NS\n",
      "[INFO] ON ticker=KOTAKBANK.NS\n",
      "[INFO] ON ticker=SBIN.NS\n",
      "[INFO] ON ticker=MARUTI.NS\n"
     ]
    }
   ],
   "source": [
    "DATAFILE='../algodata/realdata/alldata.csv'\n",
    "print(f'Reading datafile: {DATAFILE}')\n",
    "df=pd.read_csv(DATAFILE)\n",
    "\n",
    "if 'Date' not in df.columns: \n",
    "    print('Adding Date')\n",
    "    df['Date']=df.apply(stringify,axis=1)\n",
    "\n",
    "print('Creating feed')\n",
    "data=pd.read_csv('./capvolfiltered.csv')\n",
    "tickers=[t for t in list(df['ticker'].unique()) if t in list(data['ticker'].values)]\n",
    "print(tickers)\n",
    "feed=DataFeed(tickers=tickers,dfgiven=True,df=df)\n",
    "\n",
    "print('Processing feed')\n",
    "add_addl_features_feed(feed,tickers=feed.tickers)\n",
    "add_sym_feature_feed(feed,tickers=feed.tickers)\n",
    "\n",
    "\n",
    "print('Adding features to feed!')\n",
    "for ticker in feed.data:\n",
    "    print(f'[INFO] ON ticker={ticker}')\n",
    "    df = feed.data[ticker]\n",
    "    df, pre_discrete_cols, discrete_cols, thresholds = add_features(df, columns_to_use=cols_to_use)\n",
    "    complete_thresh[ticker] = thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_thresh = {}\n",
    "columns = complete_thresh['AARTIDRUGS.NS'].keys()\n",
    "\n",
    "for col in columns:\n",
    "    ls, hs = [], []\n",
    "    for ticker in complete_thresh:\n",
    "        l, h = complete_thresh[ticker][col]\n",
    "        ls.append(l); hs.append(h)\n",
    "    final_thresh[col] = (np.mean(ls), np.mean(hs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete_thresh['WIPRO.NS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "extra_tickers = [t for t in list(data['ticker'].values) if t not in tickers]\n",
    "# extra_tickers = list(np.random.choice(extra_tickers, size=20, replace=False))\n",
    "etra_tickers = {'tickers': extra_tickers}\n",
    "with open('extra_ticker.json', 'w') as f:\n",
    "    json.dump(extra_tickers, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NTPC.NS', 'DCM.NS', 'ACC.NS', 'TECHM.NS', 'MRF.NS', 'SUNPHARMA.NS', 'BPCL.NS', 'RELIANCE.NS', 'MUTHOOTFIN.NS', 'HINDUNILVR.NS', 'HDFC.NS', 'POWERGRID.NS', 'WIPRO.NS', 'PFC.NS', 'PETRONET.NS', 'DRREDDY.NS', 'ABBOTINDIA.NS', 'TCS.NS', 'BOSCHLTD.NS', 'KOTAKBANK.NS', 'SBIN.NS', 'MARUTI.NS']\n"
     ]
    }
   ],
   "source": [
    "print(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DMART.NS', 'NESTLEIND.NS', 'ULTRACEMCO.NS', 'SBILIFE.NS', 'ICICIBANK.NS', 'BAJAJ-AUTO.NS', 'HDFCLIFE.NS', 'COALINDIA.NS', 'BAJAJFINSV.NS', 'HDFCBANK.NS', 'BAJFINANCE.NS', 'ICICIPRULI.NS', 'IOC.NS', 'SBICARD.NS', 'SHREECEM.NS', 'LT.NS', 'ICICIGI.NS', 'ONGC.NS', 'PGHH.NS', 'HAL.NS', 'BAJAJHLDNG.NS', 'PAGEIND.NS', 'GILLETTE.NS', 'ITC.NS', 'INDIGO.NS', 'AXISBANK.NS', 'INDUSINDBK.NS', 'ASIANPAINT.NS', 'OBCL.NS', 'HONAUT.NS', 'SANOFI.NS', 'NDTV.NS', 'COLPAL.NS']\n"
     ]
    }
   ],
   "source": [
    "print(extra_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t for t in list(df['ticker'].values) if t not in data['ticker'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading datafile: ../algodata/realdata/alldata.csv\n"
     ]
    }
   ],
   "source": [
    "DATAFILE='../algodata/realdata/alldata.csv'\n",
    "print(f'Reading datafile: {DATAFILE}')\n",
    "df=pd.read_csv(DATAFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ticker'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NELCAST.NS', 'AARTIDRUGS.NS', 'CYIENT.NS', 'HINDZINC.NS', 'TRENT.NS', 'TATAELXSI.NS', 'MINDTREE.NS', 'TI.NS', 'OBEROIRLTY.NS', 'SOMANYCERA.NS', 'CINEVISTA.NS', 'BAYERCROP.NS', 'VLSFINANCE.NS', 'GAIL.NS', 'WOCKPHARMA.NS', 'LOKESHMACH.NS', 'DBREALTY.NS', 'JKIL.NS', 'UTTAMSTL.NS', 'QUICKHEAL.NS', 'KOTHARIPRO.NS', 'RAMANEWS.NS', 'JBFIND.NS', 'SANWARIA.NS', 'SHYAMCENT.NS', 'WENDT.NS', 'DAAWAT.NS', 'PRAJIND.NS', 'JAGRAN.NS', 'UMESLTD.NS', 'TREEHOUSE.NS', 'ORIENTCEM.NS', 'OMAXE.NS', 'EKC.NS', 'INDOCO.NS', 'MTNL.NS', 'TRIGYN.NS', 'INDRAMEDCO.NS', 'VIPIND.NS', 'KKCL.NS', 'CYBERMEDIA.NS', 'CENTUM.NS', 'TATAMTRDVR.NS', 'NECCLTD.NS', 'CUBEXTUB.NS', 'KPRMILL.NS', 'SCHNEIDER.NS', 'SATIN.NS', 'ACCELYA.NS', 'ZUARIGLOB.NS', 'VHL.NS', 'ADSL.NS', 'NMDC.NS', 'NIITLTD.NS', 'MAGNUM.NS', 'OIL.NS', 'KSCL.NS', 'IRB.NS', 'DLF.NS', 'SUDARSCHEM.NS', 'PFS.NS', 'CTE.NS', 'GANESHHOUC.NS', 'MBECL.NS', 'MANAKSTEEL.NS', 'LICHSGFIN.NS', 'WELENT.NS', 'CENTENKA.NS', 'PRECOT.NS', 'HMVL.NS', 'MARICO.NS', 'PUNJLLOYD.NS', 'ABB.NS', 'LAOPALA.NS', 'CPSEETF.NS', 'DSKULKARNI.NS', 'ONWARDTEC.NS', 'INFOMEDIA.NS', 'CUMMINSIND.NS', 'DPSCLTD.NS', 'DHARSUGAR.NS', 'UFLEX.NS', 'WELSPUNIND.NS', 'PUNJABCHEM.NS', 'TATACHEM.NS', 'KIRLOSENG.NS', 'XPROINDIA.NS', 'DABUR.NS', 'UMANGDAIRY.NS', 'DHAMPURSUG.NS', 'AIL.NS', 'TIDEWATER.NS', 'SICAL.NS', 'KCPSUGIND.NS', 'ALMONDZ.NS', 'VOLTAS.NS', 'NHPC.NS', 'CENTURYPLY.NS', 'UNIENTER.NS', 'HATHWAY.NS', 'JYOTHYLAB.NS', 'ZODIACLOTH.NS', 'ADANIENT.NS', 'NBCC.NS', 'MELSTAR.NS', 'DHANBANK.NS', 'SABTN.NS', 'SHARDACROP.NS', 'PRIMESECU.NS', 'JINDALPHOT.NS', 'QGOLDHALF.NS', 'AUSTRAL.NS', 'KEC.NS', 'AUROPHARMA.NS', 'ZUARI.NS', 'CENTURYTEX.NS', 'HIRECT.NS', 'DCMSHRIRAM.NS', 'CREATIVEYE.NS', 'NELCO.NS', 'CREST.NS', 'TV18BRDCST.NS', 'RELINFRA.NS', 'CONCOR.NS', 'MIRZAINT.NS', 'DISHTV.NS', 'OFSS.NS', 'ZYDUSWELL.NS', 'FACT.NS', 'GOENKA.NS', 'TBZ.NS', 'LUPIN.NS', 'RMMIL.NS', 'GEECEE.NS', 'MAHINDCIE.NS', 'TCIDEVELOP.NS', 'WHIRLPOOL.NS', 'ABFRL.NS', 'MAHABANK.NS', 'TITAN.NS', 'JKLAKSHMI.NS', 'VIMTALABS.NS', 'VISAKAIND.NS', 'PEL.NS', 'CLNINDIA.NS', 'DCBBANK.NS', 'CIGNITITEC.NS', 'ZENSARTECH.NS', 'ZEEMEDIA.NS', 'KNRCON.NS', 'ZENTEC.NS', 'JINDALSAW.NS', 'PCJEWELLER.NS', 'VSTIND.NS', 'DCW.NS', 'HINDPETRO.NS', 'GRAVITA.NS', 'ASHOKLEY.NS', 'VOLTAMP.NS', 'ABAN.NS', 'ADVANIHOTR.NS', 'JSWSTEEL.NS', 'DHUNINV.NS', 'JKPAPER.NS', 'IMPAL.NS', 'JPOLYINVST.NS', 'BHARTIARTL.NS', 'ACE.NS', 'OMAXAUTO.NS', 'AARVEEDEN.NS', 'VSTTILLERS.NS', 'GNFC.NS', 'SKFINDIA.NS', 'VADILALIND.NS', 'ORIENTABRA.NS', 'WANBURY.NS', 'TRF.NS', 'KANSAINER.NS', 'NOCIL.NS', 'JUNIORBEES.NS', 'SSWL.NS', 'J&KBANK.NS', 'VESUVIUS.NS', 'PNC.NS', 'CRISIL.NS', 'CIPLA.NS', 'MAHLIFE.NS', 'KAUSHALYA.NS', 'RECLTD.NS', 'BANKINDIA.NS', 'HIL.NS', 'KITEX.NS', 'JAMNAAUTO.NS', 'AARTIIND.NS', 'JKTYRE.NS', 'DBCORP.NS', 'VAIBHAVGBL.NS', 'KSL.NS', 'WABCOINDIA.NS', 'ASIL.NS', 'GSCLCEMENT.NS', 'MCX.NS', 'CYBERTECH.NS', 'DELTACORP.NS', 'ORIENTBELL.NS', 'SAMBHAAV.NS', 'LUXIND.NS', 'SWARAJENG.NS', 'ADANIPORTS.NS', 'VINYLINDIA.NS', 'XCHANGING.NS', 'JMA.NS', 'TEXRAIL.NS', 'ZEEL.NS', 'BANG.NS', 'JAYAGROGN.NS', 'UGARSUGAR.NS', 'KAJARIACER.NS', 'SIEMENS.NS', 'JINDWORLD.NS', 'JUBLFOOD.NS', 'INDIANCARD.NS', 'USHAMART.NS', 'MBLINFRA.NS', 'HDIL.NS', 'TATAMOTORS.NS', 'JUBLINDS.NS', 'SOUTHBANK.NS', 'UNITECH.NS', 'SAKHTISUG.NS', 'VALECHAENG.NS', 'LPDC.NS', 'MINDAIND.NS', 'ICIL.NS', 'SASKEN.NS', 'JAIBALAJI.NS', 'UNITEDTEA.NS', 'INDIACEM.NS', 'WONDERLA.NS', 'CUB.NS', 'NCLIND.NS', 'JISLDVREQS.NS', 'UFO.NS', 'COFFEEDAY.NS', 'RATNAMANI.NS', 'IDBI.NS', 'CGCL.NS', 'TATASTEEL.NS', 'SHILPAMED.NS', 'CINELINE.NS', 'RAJSREESUG.NS', 'WEBELSOLAR.NS', 'WSTCSTPAPR.NS', 'HCC.NS', 'JKCEMENT.NS', 'VARDHACRLC.NS', 'JSWENERGY.NS', 'KOTARISUG.NS', 'DATAMATICS.NS', 'ABHISHEK.NS', 'VMART.NS', 'ZEELEARN.NS', 'MEGASOFT.NS', 'NDL.NS', 'PARASPETRO.NS', 'WILLAMAGOR.NS', 'TCI.NS', 'JPPOWER.NS', 'BVCL.NS', 'PATINTLOG.NS', 'VRLLOG.NS', 'SAGCEM.NS', 'JISLJALEQS.NS', 'DECCANCE.NS', 'VASWANI.NS', 'ROHLTD.NS', 'DTIL.NS', 'KAKATCEM.NS', 'QNIFTY.NS', 'ENERGYDEV.NS', 'ADANITRANS.NS', 'TIRUMALCHM.NS', 'JSWHL.NS', 'GSPL.NS', 'SUBROS.NS', 'JAICORPLTD.NS', 'JUSTDIAL.NS', 'YESBANK.NS', 'CEREBRAINT.NS', 'UNIONBANK.NS', 'SHREYAS.NS', 'MCDOWELL-N.NS', 'ZENITHEXPO.NS', 'KOPRAN.NS']\n"
     ]
    }
   ],
   "source": [
    "print([t for t in list(df['ticker'].unique()) if t not in data['ticker'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TV18BRDCST.NS    12078\n",
       "TRENT.NS         12064\n",
       "WOCKPHARMA.NS    12024\n",
       "SIEMENS.NS       12015\n",
       "BHARTIARTL.NS    12005\n",
       "AARTIIND.NS      12005\n",
       "TATACHEM.NS      12004\n",
       "WIPRO.NS         12004\n",
       "VOLTAS.NS        12002\n",
       "MUTHOOTFIN.NS    11934\n",
       "PFC.NS           11933\n",
       "PEL.NS           11933\n",
       "JUNIORBEES.NS    11931\n",
       "SBIN.NS          11930\n",
       "TATAMTRDVR.NS    11929\n",
       "DABUR.NS         11929\n",
       "BANKINDIA.NS     11904\n",
       "LICHSGFIN.NS     11863\n",
       "TATAMOTORS.NS    11856\n",
       "ZEEL.NS          11856\n",
       "Name: ticker, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['ticker'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([t for t in list(data['ticker'].values) if t not in df['ticker'].unique()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metarl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
